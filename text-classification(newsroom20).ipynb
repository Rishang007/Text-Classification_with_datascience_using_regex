{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkAESvxT-8D8",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification (Newsroom20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3e5VPO5-8D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvH-L9Jd-8EF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk import ne_chunk, word_tokenize, pos_tag, Tree\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSlCDlwNbIc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Wq_CBOyhqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix = '/content/drive/My Drive/documents/' "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P1yeWrkyhqO",
        "colab_type": "text"
      },
      "source": [
        "# Creating a preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcgaJO3RyhqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(path):\n",
        "    #global variables\n",
        "    email=[]\n",
        "    label = []\n",
        "    doc_num = []\n",
        "    subject = []\n",
        "    text = []\n",
        "    \n",
        "    #getting class name\n",
        "    for filename in os.listdir(path):\n",
        "        i,j = filename.split('_')\n",
        "        j = int(j.split('.')[0])\n",
        "        label.append(i)\n",
        "        doc_num.append(j)\n",
        "        \n",
        "    #for each document in the corpus\n",
        "    for filename in tqdm(os.listdir(path)):\n",
        "        \n",
        "        file = open(path+filename,'r+')\n",
        "        f =file.read()\n",
        "        \n",
        "        em = [] # for each doc\n",
        "        st = \"\"  # for each doc\n",
        "        \n",
        "        #extracting email\n",
        "        for i in re.findall(r'[\\w\\-\\.]+@[\\w\\.-]+\\b', f):  #for every email in the doc\n",
        "            temp=[]\n",
        "            temp = i.split('@')[1]\n",
        "            temp = temp.split('.')\n",
        "            if 'com' in temp:\n",
        "                temp.remove('com')\n",
        "            for i in temp:\n",
        "                if len(i)>2:\n",
        "                    em.append(i)\n",
        "\n",
        "        for i in em:\n",
        "            st+=i\n",
        "            st+=' '\n",
        "        \n",
        "        #extracting subject\n",
        "        temp1 = re.findall(r'^Subject.*$',f, re.MULTILINE)\n",
        "        sub = temp1[0]\n",
        "        sub = sub[7:]\n",
        "        \n",
        "        for i in string.punctuation:\n",
        "            sub = sub.replace(i,\" \")\n",
        "        sub = re.sub(r\"re\",\"\",sub, flags=re.IGNORECASE)\n",
        "        sub = sub.lower()\n",
        "        \n",
        "        #appending the extracted data to a list\n",
        "        email.append(st)\n",
        "        subject.append(sub)\n",
        "        \n",
        "        \n",
        "        f = re.sub(r'[\\w\\-\\.]+@[\\w\\.-]+\\b',' ',f)                      #replace email with space\n",
        "        f = re.sub(r'Subject.*$',\" \",f, flags=re.MULTILINE)            #replace subject with space\n",
        "        f = re.sub(r\"Write to:.*$\",\"\",f, flags=re.MULTILINE)           #replace write to with nothing \n",
        "        f = re.sub(r\"From:.*$\",\"\",f, flags=re.MULTILINE)               #replace from with nothing \n",
        "        f = re.sub(r\"or:\",\"\",f,flags=re.MULTILINE)\n",
        "        f = re.sub(r\"<.*>\",\"\",f, flags=re.MULTILINE)                   #delete <anyword>    \n",
        "        f = re.sub(r\"\\(.*\\)\",\"\",f,flags=re.MULTILINE)                  #delete (contents)\n",
        "        f = re.sub(r\".*:\",\"\",f, flags=re.MULTILINE)                    #delete Anyword:\n",
        "        f = re.sub(r\"[\\n\\t\\-\\\\\\/]\",\" \",f, flags=re.MULTILINE)          #delete /,-,/n,/t\n",
        "        \n",
        "        # decontraction\n",
        "        \n",
        "        # specific\n",
        "        f = re.sub(r\"won't\", \"will not\", f)\n",
        "        f = re.sub(r\"can\\'t\", \"can not\", f)\n",
        "        # general\n",
        "        f = re.sub(r\"n\\'t\", \" not\", f)\n",
        "        f = re.sub(r\"\\'re\", \" are\", f)\n",
        "        f = re.sub(r\"\\'s\", \" is\", f)\n",
        "        f = re.sub(r\"\\'d\", \" would\", f)\n",
        "        f = re.sub(r\"\\'ll\", \" will\", f)\n",
        "        f = re.sub(r\"\\'t\", \" not\", f)\n",
        "        f = re.sub(r\"\\'ve\", \" have\", f)\n",
        "        f = re.sub(r\"\\'m\", \" am\", f)\n",
        "\n",
        "        \n",
        "        #chunking\n",
        "        chunks=[]\n",
        "        chunks=(list(ne_chunk(pos_tag(word_tokenize(f)))))\n",
        "\n",
        "        for i in chunks:\n",
        "            if type(i)==Tree:\n",
        "\n",
        "                if i.label() == \"GPE\":\n",
        "                    j = i.leaves()\n",
        "\n",
        "                    if len(j)>1:                                 #if new york or bigger name \n",
        "                        gpe = \"_\".join([term for term,pos in j])\n",
        "                        #print(gpe)\n",
        "                        f = re.sub(rf'{j[1][0]}',gpe,f, flags=re.MULTILINE)  #replacing york with new_york\n",
        "                        f = re.sub(rf'\\b{j[0][0]}\\b',\"\",f, flags=re.MULTILINE) #deleting new,  \\b is important\n",
        "\n",
        "                if i.label()==\"PERSON\":                                         #removing person\n",
        "                    for term,pog in i.leaves():\n",
        "                        f = re.sub(re.escape(term),\"\",f, flags=re.MULTILINE)\n",
        "\n",
        "                        \n",
        "        f = re.sub(r'\\d',\"\",f, flags=re.MULTILINE)                               #remove digits\n",
        "        f = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",f)                                 #replace _word_ to word\n",
        "        f = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",f)                                  #replace_word to word\n",
        "        f = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",f)                                  #replace word_ to word\n",
        "        f = re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)\",r\"\\1\",f)                         #d_berlin to berlin\n",
        "        f = re.sub(r\"\\b[a-zA-Z]{2}_([a-zA-Z]+)\",r\"\\1\",f)                         #mr_cat to cat\n",
        "        f = f.lower()                                                            #lower case\n",
        "        f = re.sub(r'\\b\\w{1,2}\\b',\" \",f)                                         #remove words <2\n",
        "        f = re.sub(r\"\\b\\w{15,}\\b\",\" \",f)                                         #remove words >15\n",
        "        f = re.sub(r\"[^a-zA-Z_]\",\" \",f)                                          #keep only alphabets and _\n",
        "        \n",
        "        f = re.sub(r\" {2,}\", \" \", f, flags=re.MULTILINE)                         # REMOVE THE EXTRA SPACES\n",
        "\n",
        "        text.append(f)\n",
        "        \n",
        "        file.close()\n",
        "        \n",
        "    \n",
        "\n",
        "    return doc_num, label, email, subject, text\n",
        "        \n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9_qcibfyhqa",
        "colab_type": "text"
      },
      "source": [
        "#### Note: to verify preprocessing function, put break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooEwon1Kyhqb",
        "colab_type": "text"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tPAtM8CNyhqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_num, label, email, subject, text = preprocessing(prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJe3ZKmbN8Uf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f25861d9-3c8e-41d2-a312-8ce9e2eb436b"
      },
      "source": [
        "DATA.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>doc_num</th>\n",
              "      <th>class</th>\n",
              "      <th>Pre_Email</th>\n",
              "      <th>Pre_Subject</th>\n",
              "      <th>Pre_text</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>49960</td>\n",
              "      <td>12</td>\n",
              "      <td>mantis netcom mantis</td>\n",
              "      <td>alt atheism faq  atheist sources</td>\n",
              "      <td>atheism resources resources december atheist ...</td>\n",
              "      <td>mantis netcom mantis   alt atheism faq  atheis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>51060</td>\n",
              "      <td>12</td>\n",
              "      <td>mantis mantis mantis</td>\n",
              "      <td>alt atheism faq  introduction to atheism</td>\n",
              "      <td>atheism introduction introduction pril egin p...</td>\n",
              "      <td>mantis mantis mantis   alt atheism faq  introd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>51119</td>\n",
              "      <td>12</td>\n",
              "      <td>dbstu1 tu-bs mimsy umd edu umd edu</td>\n",
              "      <td>gospel dating</td>\n",
              "      <td>article well has quite different not necessar...</td>\n",
              "      <td>dbstu1 tu-bs mimsy umd edu umd edu     gospel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>51120</td>\n",
              "      <td>12</td>\n",
              "      <td>mantis kepler unh edu</td>\n",
              "      <td>university violating separation of church ...</td>\n",
              "      <td>recently ras have been ordered and none have ...</td>\n",
              "      <td>mantis kepler unh edu     university violating...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>51121</td>\n",
              "      <td>12</td>\n",
              "      <td>Watson Ibm Com harder ccr-p ida org harder ccr...</td>\n",
              "      <td>soc motss  et al    princeton axes matchi...</td>\n",
              "      <td>however hate economic terrorism and political...</td>\n",
              "      <td>Watson Ibm Com harder ccr-p ida org harder ccr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                  preprocessed_text\n",
              "0           0  ...  mantis netcom mantis   alt atheism faq  atheis...\n",
              "1           1  ...  mantis mantis mantis   alt atheism faq  introd...\n",
              "2           2  ...  dbstu1 tu-bs mimsy umd edu umd edu     gospel ...\n",
              "3           3  ...  mantis kepler unh edu     university violating...\n",
              "4           4  ...  Watson Ibm Com harder ccr-p ida org harder ccr...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BZSQoHLyhq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = DATA['preprocessed_text']\n",
        "Y = DATA['class']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK9yAQD4yhq9",
        "colab_type": "text"
      },
      "source": [
        "### Splitting data into train, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvjx0nK6yhq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, stratify=Y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmFJOrHSyhrB",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRXssDNDyhrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO2Bg4j0yhrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')  #removing underscore from filters"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLQy4bKvyhrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.fit_on_texts(x_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeUMU30tyhrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(t.word_index) + 1   # to give 0 index for unknown words."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXU3gJj7yhrR",
        "colab_type": "text"
      },
      "source": [
        "#### Train Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pFJy4xU-yhrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text_train = t.texts_to_sequences(x_train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxRISQ6AyhrV",
        "colab_type": "text"
      },
      "source": [
        "#### Test Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdK-PyRMyhrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text_test = t.texts_to_sequences(x_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvB2YkPTyhrZ",
        "colab_type": "text"
      },
      "source": [
        "## Getting the maximum length for padding the documents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pOwCiVdyhra",
        "colab_type": "text"
      },
      "source": [
        "**texts_to_sequences() returns nothing for new words came across in test data, padding solves the problem by adding zero to them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhvOJZtVyhrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bd27aeb-10e8-4447-da32-44332640d3dd"
      },
      "source": [
        "maxx=0\n",
        "for i in encoded_text_train:\n",
        "    if len(i)>maxx:\n",
        "        maxx = len(i)\n",
        "maxx                              #this is now the length of each document in our corpus"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZJRro1yhrd",
        "colab_type": "text"
      },
      "source": [
        "#### Train Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI6Y84-kyhre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "cc910ea5-1f90-445f-9f98-e3a27dbf3844"
      },
      "source": [
        "padded_text_train = pad_sequences(encoded_text_train, maxlen=maxx, padding='post')\n",
        "print(padded_text_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  950    10  8427 ...     0     0     0]\n",
            " [  278     1   724 ...     0     0     0]\n",
            " [19486 11082 44004 ...     0     0     0]\n",
            " ...\n",
            " [ 4427  9236 72622 ...     0     0     0]\n",
            " [ 1778  1776    10 ...     0     0     0]\n",
            " [72623  1794    10 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chY-e3cwyhrg",
        "colab_type": "text"
      },
      "source": [
        "#### Test Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff9Zi2ucyhrh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "ecaa5a8a-214e-4d05-daf4-e88fbdac58ab"
      },
      "source": [
        "padded_text_test = pad_sequences(encoded_text_test, maxlen=maxx, padding='post')\n",
        "padded_text_test"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[32204,  7847,   194, ...,     0,     0,     0],\n",
              "       [ 2766,  1947,    10, ...,     0,     0,     0],\n",
              "       [ 1690,   484,     5, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [51796, 28352,    10, ...,     0,     0,     0],\n",
              "       [ 6325,  5319,    10, ...,     0,     0,     0],\n",
              "       [ 3002,  2804,  2250, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idUfWmcIyhrk",
        "colab_type": "text"
      },
      "source": [
        "## Converting class to one hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTBXretdyhrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC89_jMkyhrq",
        "colab_type": "text"
      },
      "source": [
        "#### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVjQ5jGyhrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "be02ee33-2a0b-4066-cd6d-541dafe45a25"
      },
      "source": [
        "train_class = to_categorical(y_train)\n",
        "train_class"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Oe54-Qyhrx",
        "colab_type": "text"
      },
      "source": [
        "#### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CPju8smyhry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "ee7723ae-0125-4cde-88e8-489511ea2868"
      },
      "source": [
        "test_class = to_categorical(y_test)\n",
        "test_class"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu9AtR4wyhr5",
        "colab_type": "text"
      },
      "source": [
        "# Loading The Glove Weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMivjNatyhr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8592837-1a2b-436f-b4f9-4d1589cb2b64"
      },
      "source": [
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/?unapproved=520126&moderation-hash=5c11e5048633b93910ec2fcc019fce3b#comment-520126\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "glov = open('/content/drive/glove.6B/glove.6B.300d.txt', encoding='utf8')\n",
        "for line in glov:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "glov.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f29igk9Jyhr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "\n",
        "embedding_matrix_train = np.zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector_train = embeddings_index.get(word)\n",
        "    if embedding_vector_train is not None:\n",
        "        embedding_matrix_train[i] = embedding_vector_train"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUOYe3WlyhsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, concatenate, Dense, Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import tensorflow as tf"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DdGgU3EtyhsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "b088e2b8-3250-454c-beaa-ed9653c2e87b"
      },
      "source": [
        "#input is a text document of size max (padded document)\n",
        "i_layer= Input(shape=(maxx,)) \n",
        "\n",
        "#embedding layer\n",
        "embedd = Embedding(vocab_size, 300, weights=[embedding_matrix_train], input_length=maxx, trainable=False)(i_layer)\n",
        "\n",
        "#convolution 1D layer\n",
        "conv1 = Conv1D(16, kernel_size=9, activation='relu')(embedd)\n",
        "conv2 = Conv1D(16, kernel_size=6, activation='relu')(embedd)\n",
        "conv3 = Conv1D(16, kernel_size=3, activation='relu')(embedd)\n",
        "\n",
        "#concatenate the convolution layers\n",
        "concate1 = concatenate([conv1, conv2, conv3], axis=1)   #since axis 1 will have different values\n",
        "\n",
        "#maxpool 1d layer\n",
        "maxpool1 = MaxPooling1D(pool_size=4)(concate1)\n",
        "\n",
        "#convlution layer 1D\n",
        "conv4 = Conv1D(8, kernel_size=8, activation='relu')(maxpool1)\n",
        "conv5 = Conv1D(8, kernel_size=4, activation='relu')(maxpool1)\n",
        "conv6 = Conv1D(8, kernel_size=2, activation='relu')(maxpool1)\n",
        "\n",
        "#concatenate the convolution layers\n",
        "concate2 = concatenate([conv4,conv5, conv6], axis=1)\n",
        "\n",
        "#maxpool 1d layer\n",
        "maxpool2 = MaxPooling1D(pool_size=2)(concate2)\n",
        "\n",
        "#convolution 1D layer\n",
        "conv7 =  Conv1D(8, kernel_size=5, activation='relu')(maxpool2)\n",
        "\n",
        "#flatten\n",
        "flat = Flatten()(conv7)\n",
        "\n",
        "#dropout\n",
        "drop = Dropout(0.5)(flat)\n",
        "\n",
        "#dense\n",
        "dense = Dense(10, activation='relu')(drop)\n",
        "\n",
        "#output layer\n",
        "o_layer = Dense(20, activation='softmax')(dense)\n",
        "\n",
        "#define the mode\n",
        "model = Model(inputs=i_layer, outputs=o_layer)\n",
        "\n",
        "#compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#summarize the model\n",
        "print(model.summary())\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 8673)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 8673, 300)    21787200    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 8665, 16)     43216       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 8668, 16)     28816       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 8671, 16)     14416       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 26004, 16)    0           conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 6501, 16)     0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 6494, 8)      1032        max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 6498, 8)      520         max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 6500, 8)      264         max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 19492, 8)     0           conv1d_10[0][0]                  \n",
            "                                                                 conv1d_11[0][0]                  \n",
            "                                                                 conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 9746, 8)      0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 9742, 8)      328         max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 77936)        0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 77936)        0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           779370      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 20)           220         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,655,382\n",
            "Trainable params: 868,182\n",
            "Non-trainable params: 21,787,200\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLQstLTeQCPk",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0-Z2TvEPmHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime \n",
        "#tensorboard callback\n",
        "log_dir=\"/content/drive/My Drive/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #dirctory according to time\n",
        "tfboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_graph=True)\n",
        "\n",
        "#early stopping callback\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
        "\n",
        "#modelcheckpoint callback to save the best model \n",
        "filepath = '/content/drive/My Drive/weights.{epoch:02d}-{val_loss:.2f}.hdf5'    #saves in the name of epoch and val_loss\n",
        "chk_callback = tf.keras.callbacks.ModelCheckpoint(filepath,monitor='val_loss', mode='min',\n",
        "                                                 save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JowKz_m2QA5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(padded_text_train,train_class, epochs=30, validation_data=(padded_text_test,test_class), \n",
        "         callbacks=[tfboard_callback, es_callback, chk_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A95T0n_Syhsg",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard M1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sl7JrDtyhsg",
        "colab_type": "text"
      },
      "source": [
        "### Observation:\n",
        "1. **Minimum val-loss at epoch 4, so the EarlyStopping stopped the training after no improvements in 5 epochs after it.**\n",
        "2. **The weights are stored as \"weight.04-0.95.hdf5\" (epoch-4 has minimum val_loss-0.95 and accuracy at 70%)**\n",
        "3. **Red line- Validation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZtEWySByhsh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://imgur.com/QhXj351.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cis-qReqyhsh",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://imgur.com/889JQLm.png\">"
      ]
    }
  ]
}